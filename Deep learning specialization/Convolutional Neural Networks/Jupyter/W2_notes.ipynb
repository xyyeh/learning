{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dec649e-f35e-43e4-8b74-947dd5c7d87b",
   "metadata": {},
   "source": [
    "### Week 2\n",
    "#### Classic Networks\n",
    "1. **LeNet-5** is a classical CNN. Several interesting observations: (1) as we go deeper, $n_H$ and $n_W$ reduce while $n_C$ increases (2) convolution layers are punctuated with pooling layers\n",
    "2. **AlexNet** is used for ImageNet classification and is much larger than LeNet-5. It also uses ReLU activation.\n",
    "3. **VGG-16** is another classical network that uses purely convolution layers where each conv-pool layer reduces the height and width by half."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741951b-aa04-4d59-a766-786798d92f61",
   "metadata": {},
   "source": [
    "#### Practical Advice Using ConvNets\n",
    "1. Transfer learning can be done by freezing weights of existing layers and train only the parameters of the newly added layers. If you have a large dataset, you can choose to unfreeze several downstream layers and use them to train with the new dataset. You can also choose to drop the last few layers and add in your new layers. This provides better flexibility as you can use different layer configurations. In the event that you have a very large dataset, you can use the existing weights as initialization and retrain the whole network.\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
