{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058abadb-0f5b-46bd-9cd7-11b9d0cf7b1d",
   "metadata": {},
   "source": [
    "## Neural Networks and Deep Learning\n",
    "### Supervised learning\n",
    "1. Train a function approximator based on inputs to predict an output\n",
    "2. Image - usese CNN\n",
    "3. Audio/translation - RNN\n",
    "4. Autonomous driving - custom/hybrid NN\n",
    "5. Structure data - input/output databases; Unstructured data - raw audio, images, text\n",
    "6. One of the huge breakthroughs of NN is the switching of sigmoid to ReLU function. The signmoid function has close to zero gradients at both ends that slows down gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3aa173-84f5-4544-9927-b73be5a9a478",
   "metadata": {},
   "source": [
    "## Binary classifiications\n",
    "1. Based on two classes.\n",
    "2. Typical notiation, training inputs $X$, $X\\in \\mathbb{R}^{n_x \\times m}$ where all the training data is spanning the columns. For the output, $Y$ is also written as a concatenation of columns.\n",
    "\n",
    "### Logistic regression\n",
    "1. Given $x$, we want $\\hat y = P(y = 1 | x) \\in \\{0,1\\}$. In linear regression, we have $y = w^T x + b$, but for logistic regression, we will use the sigmoid function, i.e. $\\hat y = \\sigma (w^T x + b)$,   $\\sigma (z) = \\frac{1}{1+e^{-z}}$.\n",
    "2. Cost function:  the loss function for logistic regression is such that $L(\\hat y, y) = - (y log \\hat y + (1-y) log (1-\\hat y)$ (https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d) (https://web.stanford.edu/~jurafsky/slp3/5.pdf)\n",
    "3. The overall loss function is defined as the sum of all errors over all the training examples, i.e. $J (w,b) = 1/m \\Sigma_{i=1..m} L(\\hat y_i, y_i)$\n",
    "4. Logistic regression can be solved using gradient discent, for example using $w = w - \\alpha \\frac{\\partial J (w,b)}{\\partial w}$ and vice versa for $b$\n",
    "\n",
    "### Computation graph for gradient descent on logistic regression\n",
    "1. Computation graph is very useful to setting up automatic computations of derivatives\n",
    "2. Suppose $x \\in \\mathbb{R}^2$, we thus have the corresponding parameters $\\{ w_1, w_2 \\}$ and $b$. We setup the computation graph with $w_1, w_2, b, x_1, x_2$, $z = w_1 x_1 + w_2 x_2 + b$, $a = \\sigma(z) $ and $L = L(a,y)$\n",
    "3. To train on $m$ examples, notice that the individual partial derivatives are also average of $m$ examples since the lost function is linear w.r.t. all individual parameters.\n",
    "\n",
    "### Vectorization\n",
    "1. In numpy, this is done with the \".dot\" function (example below). You could also use SIMD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd63a8fd-45e2-4832-9c48-8f15fda16dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in ms = 2.761363983154297\n",
      "Time taken in ms = 2.0134449005126953\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "c = 0\n",
    "\n",
    "tic = time.time()\n",
    "c = a.dot(b)\n",
    "toc = time.time()\n",
    "print(\"Time taken in ms = {}\".format(1000*(toc-tic)))\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(\"Time taken in ms = {}\".format(1000*(toc-tic)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780410e-4afa-4f3d-9490-f4ce05ded4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdbc98-f24e-4c9d-ad32-e959c915a67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
