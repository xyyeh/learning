{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto differentiation\n",
    "1. Auto differentiation is required in the computation of *gradients*, *jacobians* and *hessians*. Although there are many different ways to perform differentiation, namely, analytical functions and finite differentiation, this tutorial will only cover auto differentiation. Comprehensive comparison among them can be found easily on the web and will not be covered here.\n",
    "2. Lets start with a simple example of taking the *gradient* of a scalar valued function and see what is happening in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Variable x: 3.0\n",
      "Input Variable y: 4.0\n",
      "Product z: 12.0\n",
      "\n",
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: 3.0\n",
      "Gradient of z wrt x: 4.0, gradient of z wrt x: 3.0\n",
      "Differentiating wrt x\n",
      "{ lambda ; a:f32[] b:f32[]. let _:f32[] = mul a b; c:f32[] = mul 1.0 b in (c,) }\n",
      "Differentiating wrt y\n",
      "{ lambda ; a:f32[] b:f32[]. let _:f32[] = mul a b; c:f32[] = mul a 1.0 in (c,) }\n",
      "Differentiating wrt x, y\n",
      "{ lambda ; a:f32[] b:f32[]. let\n",
      "    _:f32[] = mul a b\n",
      "    c:f32[] = mul a 1.0\n",
      "    d:f32[] = mul 1.0 b\n",
      "  in (d, c) }\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import make_jaxpr\n",
    "from jax import vmap, pmap, jit\n",
    "from jax import grad, value_and_grad\n",
    "from jax.test_util import check_grads\n",
    "\n",
    "def product(x, y):\n",
    "    z = x * y\n",
    "    return z\n",
    "\n",
    "\n",
    "x = 3.0\n",
    "y = 4.0\n",
    "\n",
    "z = product(x, y)\n",
    "\n",
    "print(f\"Input Variable x: {x}\")\n",
    "print(f\"Input Variable y: {y}\")\n",
    "print(f\"Product z: {z}\\n\")\n",
    "\n",
    "# dz / dx\n",
    "dx = grad(product, argnums=0)(x, y) # diff wrt first arg, default of argnums is 0\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "\n",
    "# dz / dy\n",
    "dy = grad(product, argnums=1)(x, y) # diff wrt second arg\n",
    "print(f\"Gradient of z wrt y: {dy}\")\n",
    "\n",
    "# p = dz / d(x,y) can also be done\n",
    "p = grad(product, argnums=(0,1))(x,y)\n",
    "print(f\"Gradient of z wrt x: {p[0]}, gradient of z wrt x: {p[1]}\")\n",
    "\n",
    "print(\"Differentiating wrt x\")\n",
    "print(make_jaxpr(grad(product, argnums=0))(x,y))\n",
    "\n",
    "print(\"Differentiating wrt y\")\n",
    "print(make_jaxpr(grad(product, argnums=1))(x,y))\n",
    "\n",
    "print(\"Differentiating wrt x, y\")\n",
    "print(make_jaxpr(grad(product, argnums=(0,1)))(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Notice that the argument that we are differentiating wrt is a constant 1.0\n",
    "4. Using **`vmap`**, we can batch compute gradients of a scalar valued function at multiple points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for the batch:  [0.482287   0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Jaxpr:\n",
      " { lambda ; a:f32[5]. let\n",
      "    b:f32[5] = tanh a\n",
      "    c:f32[5] = sub 1.0 b\n",
      "    d:f32[5] = mul 1.0 c\n",
      "    e:f32[5] = mul d b\n",
      "    f:f32[5] = add_any d e\n",
      "  in (f,) }\n"
     ]
    }
   ],
   "source": [
    "def activate(x):\n",
    "    \"\"\"Applies tanh activation.\"\"\"\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "key = random.PRNGKey(1234)\n",
    "x = random.normal(key=key, shape=(5,))\n",
    "activations = activate(x)\n",
    "\n",
    "grads_batch = vmap(grad(activate))(x)\n",
    "print(\"Gradients for the batch: \", grads_batch)\n",
    "\n",
    "print(\"Jaxpr:\\n\", make_jaxpr(vmap(grad(activate)))(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Again, we can incorporate additional transformation, such as **`jit`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for the batch:  [0.482287   0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Time taken: 0.0219 second\n",
      "==================================================\n",
      "Gradients for the batch:  [0.482287   0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Time taken: 0.0007 second\n",
      "==================================================\n",
      "Gradients for the batch:  [0.482287   0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Time taken: 0.0003 second\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "jitted_grads_batch = jit(vmap(grad(activate)))\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = time.time()\n",
    "    print(\"Gradients for the batch: \", jitted_grads_batch(x))\n",
    "    print(f\"Time taken: {time.time() - start_time:.4f} second\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Jax also provides a finite difference utility function **`check_grads`** for developers to verify the computation of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient match with gradient calculated using finite differences\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_grads(jitted_grads_batch, (x,), order=1)\n",
    "    print(\"Gradient match with gradient calculated using finite differences\")\n",
    "except Exception as ex:\n",
    "    print(type(ex).__name__, ex)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a056c220e358cd254f9e086c9fbc1600e1f6c115100e0ffe1e1fae2263f3989e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hdrm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
