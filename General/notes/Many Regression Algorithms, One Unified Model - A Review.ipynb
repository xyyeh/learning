{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Regression Algorithms, One Unified Model - A Review\n",
    "1. Function approximators are often used to capture learned paths, for example, in DMPs\n",
    "2. Despite their many flavours (LWR, GPR, GMR, etc), they form a special case of the unified model\n",
    "3. This papers contribution is:\n",
    "<br> (1) A wide variety of regression algorithms fall into two main classes: a mixture of linear models or a weighted sum of basis functions\n",
    "<br> (2) The second class is a special case of the former"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares regression\n",
    "1. $a^* = arg min_a (y-Xa)^T (y-Xa)$. The solution is $a^* = (X^T X)^{-1} X^T y$\n",
    "2. $a^* = arg min_a (\\frac{\\lambda}{2} ||a||^2 + \\frac{1}{2} ||y-X^T a||^2)$, known as Thikonov regularization or Ridge regression. The solution is $a^* = (\\lambda I + X^T X)^{-1} X^T y$. The $L_1$ norm can be applied to the regularization term.\n",
    "3. These are batch learning methods. There are also incremental least squares methods, for example, recursive least squares\n",
    "4. Due ot the inversion of the matrix, the complexity is $O(n^3)$. Although the Sherman-Morrison formula can be used to reduce the inversion complexity to $O(n^2)$, the method is sensitive to rounding errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear regression\n",
    "1. Locally weighted regression (LWR) uses the cost function $S(a) = \\sum^N w_n (y_n - a^T x_n)^2 = (y-Xa)^T W (y-Xa)$ and the solution to the problem is $a^* = (X^T W X)^{-1} X^T W y$\n",
    "2. The weights for each sample are typically defined as a function of the input space thorugh a function $\\phi$ parameterized with $\\theta$, i.e. $w_n = \\phi(x_n, \\theta)$ where $\\theta$ is a fixed parameter\n",
    "3. A commonly used weighting function is the multivariate Gaussian:\n",
    "<br> $\\phi(x_n, \\theta) = g(x_n, c, \\Sigma)$ with $\\theta = (c, \\Sigma)$\n",
    "<br> $g(x,c,\\Sigma) = exp(-\\frac{1}{2}(x-c)^T \\Sigma^{-1} (x-c))$\n",
    "4. A radial basis function $f(x) = \\sum^E \\phi (x, \\theta_e)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
