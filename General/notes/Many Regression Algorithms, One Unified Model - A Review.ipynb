{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Regression Algorithms, One Unified Model - A Review\n",
    "1. Function approximators are often used to capture learned paths, for example, in DMPs\n",
    "2. Despite their many flavours (LWR, GPR, GMR, etc), they form a special case of the unified model\n",
    "3. This papers contribution is:\n",
    "<br> (1) A wide variety of regression algorithms fall into two main classes: a mixture of linear models or a weighted sum of basis functions\n",
    "<br> (2) The second class is a special case of the former"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares regression\n",
    "1. $a^* = arg min_a (y-Xa)^T (y-Xa)$. The solution is $a^* = (X^T X)^{-1} X^T y$\n",
    "2. $a^* = arg min_a (\\frac{\\lambda}{2} ||a||^2 + \\frac{1}{2} ||y-X^T a||^2)$, known as Thikonov regularization or Ridge regression. The solution is $a^* = (\\lambda I + X^T X)^{-1} X^T y$. The $L_1$ norm can be applied to the regularization term.\n",
    "3. These are batch learning methods. There are also incremental least squares methods, for example, recursive least squares\n",
    "4. Due ot the inversion of the matrix, the complexity is $O(n^3)$. Although the Sherman-Morrison formula can be used to reduce the inversion complexity to $O(n^2)$, the method is sensitive to rounding errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters vs Meta parameters\n",
    "1. Algorithms are designed to determine the optimal values of the parameters of the model, given an optimization criterion. Meta-parameters are algorithmic parameters that the *user has to provide* as an input to the algorithm. Take for example, $f = a^T x + b$, the model parameters are $a$ and $b$, they are **all** the paramters required to make a prediction for a novel output.\n",
    "2. For least squares, there are no meta-parameters; for Thikonov regularization, the user has to tune the parameter $\\lambda$. The parameter $\\lambda$ is thus a meta-parameter.\n",
    "3. Another way of looking at these parameters is that model parameters depend only on the training data. With regularized LLS, the resulting parameter $a$ depends on both the training data and the meta-parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear regression using linear methods\n",
    "1. In general there are two ways to approach nonlinear regression: (1) Algorihtms that perform multiple weighted LLS regressions, using different input-dependent weighting functions. The resulting model is a mixture of linear models. Examples include LWR, GMR, LWPR. (2) Algorithms that project the input space into a feature space using a set of non-linear basis functions, and performing one LLS regression in this projected feature space. Examples include RBFNs and KRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type (1)  Mixture of linear models\n",
    "1. The underlying model is a mixture of linear models, where the *sub-models* are linear and where the *weights* are determined by the normalized weighting functions\n",
    "2. Algorithm can be defined as $f(x) = \\sum^E \\phi(x,\\theta_e).(a_e^T x + b_e)$\n",
    "\n",
    "#### LWR\n",
    "1. Locally weighted regression (LWR) uses the cost function $S(a) = \\sum^N w_n (y_n - a^T x_n)^2 = (y-Xa)^T W (y-Xa)$ and the solution to the problem is $a^* = (X^T W X)^{-1} X^T W y$ where $W$ is a diagonal matrix\n",
    "2. The weights for each sample are typically defined as a function of the input space thorugh a function $\\phi$ parameterized with $\\theta$, i.e. $w_n = \\phi(x_n, \\theta)$ where $\\theta$ is a fixed parameter\n",
    "3. A commonly used weighting function is the multivariate Gaussian:\n",
    "<br> $\\phi(x_n, \\theta) = g(x_n, c, \\Sigma)$ with $\\theta = (c, \\Sigma)$\n",
    "<br> $g(x,c,\\Sigma) = exp(-\\frac{1}{2}(x-c)^T \\Sigma^{-1} (x-c))$\n",
    "4. LWR is an extension of the weighted linear least squares, in which $E$ independent weighted regressions are performed on the same data (in the design matrix $X$), but with $E$ independent weight matrices $W_e$:\n",
    "<br> $a_e = (X^T W_e X)^{-1} X^T W_e y, \\forall e = 1...E$\n",
    "5. The resulting model is $f(x) = \\sum^E \\phi(x,\\theta_e).(a_e^T x + b_e)$ where the basis functions are often selected as normalized gaussian weighting function $\\phi(x,\\theta_e) = \\frac{g(x,c_e,\\Sigma_e)}{\\Sigma^E g(x,c_{e'},\\Sigma_{e'})}$ with $\\theta_e = (c_e, \\Sigma_e)$\n",
    "\n",
    "#### GMR\n",
    "1. Gaussian mixture regression assumes that the data in the joint input $\\times$ output ($x-y$) space can be represented by a set of gaussians, which is known as a gaussian mixture model (GMM)\n",
    "2. A notable feature of GMR is that the training phase consists of unsupervised learning, performed by fitting a GMM to the data with the Expectation-maximization (EM) algorithm. Usually k-means clustering is used to provide a first initialization of the centers.\n",
    "3. Because EM is an unsupervised learning algorithm, there  is no distinction between an input $x_n$ and a target $y_n$. They are concatenated into one vector $z_n = [x_n^T y_n]^T$ The GMM represents a model of the density of the vectors $z_n$ as a weighted sum of $E$ gaussian functions: $p(z_n) = \\sum^E \\pi_e N(z_n; \\mu_e, \\Sigma_e)$ where $\\sum \\pi_e = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type (2) Basis function network\n",
    "1. Weighted mixture of basis functions, i.e. $f(x) = \\sum^E \\phi(x,\\theta_e).w_e$\n",
    "2. Least squares for basis function networks results in weights of the form $w^* = (Z^T Z)^{-1} Z^T y$ or $w^* = (\\lambda I + Z^T Z)^{-1} Z^T y$. $Z$ can take three forms:\n",
    "<br> Design matrix\n",
    "<br> Feature matrix\n",
    "<br> Gram matrix\n",
    "\n",
    "#### Design matrix\n",
    "$Z = X = \\begin{bmatrix} x_{1,1} & ... & x_{1,D} \\\\ ... & ... & ... \\\\ x_{N,1} & ... & x_{N,D} \\end{bmatrix} \\in \\mathbb{R}^{N \\times D}$ (smallest)\n",
    "\n",
    "#### Feature matrix\n",
    "$Z = \\Phi(X) = \\begin{bmatrix} \\phi_{x_1,\\theta_1} & ... & \\phi_{x_1,\\theta_E} \\\\ ... & ... & ... \\\\ \\phi_{x_N,\\theta_1} & ... & \\phi_{x_N,\\theta_E} \\end{bmatrix} \\in \\mathbb{R}^{N \\times E}$ (medium)\n",
    "\n",
    "#### Gram matrix (kernel functions)\n",
    "$Z = K(X,X) = \\begin{bmatrix} k_{x_1,x_1} & ... & k_{x_1,x_N} \\\\ ... & ... & ... \\\\ k_{x_N,x_1} & ... & k_{x_N,x_N} \\end{bmatrix} \\in \\mathbb{R}^{N \\times N}$ (largest)\n",
    "\n",
    "$w^* = K^{-1} y$ and $w^* = (\\lambda I + K)^{-1} y$ (kernel trick) since the gram matrix is symmetrical (and square)\n",
    "\n",
    "#### RBFN\n",
    "1. RBFN is a specialization of the function used by LWR with $w_e = b_e$ and $a_e = \\bf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples using scikitlearn\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaparameters\n",
    "1. Often, it is easier to specify the configuration of basis functions using meta-parameters. For example, centers of gaussian functions can be selected using the intersection height with nearby gaussians. If the centers are spaced equally apart, this height determines the width of the gaussian function.\n",
    "2. Metaparameters will need to be combined with the training data to form model parameters for the unified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
