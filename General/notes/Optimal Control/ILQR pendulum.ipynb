{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ILQR example\n",
    "1. The example shows how to compute the ilqr solution to a 1 dof pendulum\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete dynamics\n",
    "1. In the following the symbolic dynamics of the pendulum is defined.\n",
    "2. For convenience, lambda functions are used for:\n",
    "    - The discrete dynamics $x_{k+1} = f(u_k, x_k)$\n",
    "    - The linearized system with $A = \\frac{\\partial f}{\\partial x}$ and $B = \\frac{\\partial f}{\\partial u}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy.matrices import Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolic_dynamics_pendulum():\n",
    "    \"\"\"Symbolic dynamics for pendulum\n",
    "    \"\"\"\n",
    "    m, g, L, theta, theta_dot, u, dt = sp.symbols('m g L theta theta_dot u dt')\n",
    "\n",
    "    # inputs and states\n",
    "    inputs = Matrix([u])\n",
    "    states = Matrix([theta, theta_dot])\n",
    "\n",
    "    # dynamics for a pendulum of mass m and center of mass L\n",
    "    # ----------------\n",
    "    #        |\\\n",
    "    #        | \\\n",
    "    # \\theta |->\\\n",
    "    #        |  []\n",
    "    f = Matrix([theta_dot, (u-m*g*L*sp.sin(theta))/(m*L*L)])\n",
    "\n",
    "    # discretize the system using euler integration\n",
    "    f_disc = states + f*dt\n",
    "\n",
    "    # first derivatives wrt to (x, u)\n",
    "    f_x = f_disc.jacobian(states) # df/dx\n",
    "    f_u = f_disc.jacobian(inputs) # df/du\n",
    "\n",
    "    # second derivatives wrt to (x, u)\n",
    "    # f_xx = f_x.(states) # d2f/dx2\n",
    "    # f_uu = f_u.jacobian(states) # d2f/du2\n",
    "\n",
    "    # define parameters\n",
    "    parameters = Matrix([m,g,L])\n",
    "\n",
    "    # create lambdas\n",
    "    f_func = sp.lambdify((states, inputs, dt, parameters), f_disc)\n",
    "    f_x_func = sp.lambdify((states, inputs, dt, parameters), f_x)\n",
    "    f_u_func = sp.lambdify((states, inputs, dt, parameters), f_u)\n",
    "    # f_xx_func = sp.lambdify((states, inputs, dt, parameters), f_xx)\n",
    "    # f_uu_func = sp.lambdify((states, inputs, dt, parameters), f_uu)\n",
    "\n",
    "    # return (f_func, f_x_func, f_u_func, f_xx_func, f_uu_func)\n",
    "    return (f_func, f_x_func, f_u_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ilqr:\n",
    "    def __init__(self, init_state, target_state, initial_guess, dt, start_time, end_time, f_discrete, A, B, Q_k, R_k, Q_T, parameters, n_iterations):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            init_state ([type]): [description]\n",
    "            target_state ([type]): [description]\n",
    "            initial_guess ([type]): [description]\n",
    "            dt ([type]): [description]\n",
    "            start_time ([type]): [description]\n",
    "            end_time ([type]): [description]\n",
    "            f_discrete ([type]): [description]\n",
    "            A ([type]): [description]\n",
    "            B ([type]): [description]\n",
    "            Q_k ([type]): [description]\n",
    "            R_k ([type]): [description]\n",
    "            Q_T ([type]): [description]\n",
    "            parameters ([type]): [description]\n",
    "            n_iterations ([type]): [description]\n",
    "        \"\"\"\n",
    "        # states\n",
    "        self.init_state_ = init_state\n",
    "        self.target_state_ = target_state\n",
    "        self.inputs_ = initial_guess\n",
    "        self.n_states_ = np.shape(init_state)[0] # The dimensions of the state vector\n",
    "        self.n_inputs_ = np.shape(initial_guess)[1] # The dimension of the control vector\n",
    "\n",
    "        # timing\n",
    "        self.dt_ = dt\n",
    "        self.start_time_ = start_time\n",
    "        self.end_time_ = end_time\n",
    "        self.time_span_ = np.arange(start_time, end_time, dt).flatten()\n",
    "        self.n_timesteps_ = np.shape(self.time_span_)[0]\n",
    "\n",
    "        # dynamics\n",
    "        self.f_ = f_discrete\n",
    "        self.A_ = A\n",
    "        self.B_ = B\n",
    "\n",
    "        # weighting for loss function, i.e. L = x_T^T Q_T x_T + sum of (x_k^T Q_k x_k + u_k^T R_k u_k)\n",
    "        self.Q_k_ = Q_k # Weight for state vector\n",
    "        self.R_k_ = R_k # Weight for control vector\n",
    "        self.Q_T_ = Q_T # Weight for terminal state\n",
    "        self.parameters_ = parameters\n",
    "\n",
    "        # max iterations to run\n",
    "        self.n_iterations_ = n_iterations\n",
    "\n",
    "        # costs\n",
    "        self.expected_cost_reduction_ = 0\n",
    "        self.expected_cost_reduction_grad_ = 0\n",
    "        self.expected_cost_reduction_hess_ = 0\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"rollout of the simulated system given an initial state\n",
    "        \"\"\"\n",
    "        # we store states and inputs as:\n",
    "        # state = [., x_1, x_2, ..., x_N]\n",
    "        # input =    [u_0, u_1, ..., u_{N-1}]\n",
    "        # the first value in state is understood as self.init_state_\n",
    "        states = np.zeros((self.n_timesteps_+1, self.n_states_)) # including initial state, x_0 to x_N\n",
    "        inputs = np.zeros((self.n_timesteps_, self.n_inputs_)) # u_0 to u_{N-1}\n",
    "        current_state = self.init_state_\n",
    "\n",
    "        # print(current_state)\n",
    "        # print(self.inputs_[0, :])\n",
    "        # print(self.dt_)\n",
    "\n",
    "        next_state = self.f_(current_state, self.inputs_[0, :],\n",
    "                             self.dt_, self.parameters_).flatten()\n",
    "        print(next_state)\n",
    "\n",
    "        for i in range(0, self.n_timesteps_): # 0 to N-1\n",
    "            current_input = self.inputs_[i,:] # u_k\n",
    "            next_state = self.f_(current_state, current_input, self.dt_, self.parameters_).flatten() # x_{k+1} = f(x_k, u_k) \n",
    "            # store both u_k and x_{k+1}\n",
    "            states[i+1,:] = next_state\n",
    "            inputs[i,:] = current_input\n",
    "            # update current state\n",
    "            current_state = next_state\n",
    "\n",
    "            if (i == 0):\n",
    "                print(next_state)\n",
    "\n",
    "        # print(states[-1, :])\n",
    "        # print(self.f_)\n",
    "        # print(self.parameters_)\n",
    "\n",
    "        # store trajectories\n",
    "        self.states_ = states\n",
    "        self.inputs_ = inputs\n",
    "\n",
    "        return states, inputs\n",
    "\n",
    "    def compute_cost(self, states, inputs):\n",
    "        \"\"\"Computes the cost from all the terms, i.e. dynamics and cost as well as their derivatives:\n",
    "        f_x, f_u, f_xx, f_ux, f_uu, \n",
    "        l_x, l_u, l_xx, l_ux, l_uu\n",
    "\n",
    "        Args:\n",
    "            states (ndarray): State trajectory\n",
    "            inputs (ndarray): Input trajectory\n",
    "\n",
    "        Returns:\n",
    "            double: Total cost, i.e. terminal cost + running cost\n",
    "        \"\"\"\n",
    "        # dynamics first derivatives\n",
    "\n",
    "        # dynamics second derivatives\n",
    "\n",
    "        # cost first derivatives\n",
    "\n",
    "        # cost second derivatives\n",
    "\n",
    "        # accumulate cost to go\n",
    "        total_cost = 0\n",
    "        for i in range(0, self.n_timesteps_):\n",
    "            current_x = states[i,:]\n",
    "            current_u = inputs[i,:].flatten()\n",
    "            current_cost = current_u.T @ self.R_k_ @ current_u\n",
    "            total_cost = total_cost + current_cost\n",
    "        # add terminal cost\n",
    "        terminal_diff = (states[-1,:] - self.target_state_).flatten()\n",
    "        terminal_cost = terminal_diff.T @ self.Q_T_ @ terminal_diff\n",
    "        total_cost = total_cost + terminal_cost\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    def backward_pass(self):\n",
    "        \"\"\"Backward pass of iLQR\n",
    "\n",
    "        Returns:\n",
    "            ndarray: feedforward gain, k\n",
    "            ndarray: feedback gain, K\n",
    "            double: expected cost reduction\n",
    "        \"\"\"\n",
    "        # starting from the last state\n",
    "        V_xx = self.Q_T_ # since V_N = x_T^T Q_T x_T, V_xx(N) = Q_T_\n",
    "        end_difference = (self.states_[-1, :] - self.target_state_).flatten()\n",
    "        end_difference = end_difference.flatten()\n",
    "        V_x = self.Q_T_ @ end_difference # V_x(N)\n",
    "\n",
    "        # initialize control modifications to be stored\n",
    "        k_trj = np.zeros((self.n_timesteps_, self.n_inputs_)) # (8b)\n",
    "        K_trj = np.zeros((self.n_timesteps_, self.n_inputs_, self.n_states_)) # (8b)\n",
    "\n",
    "        # initialize cost reduction\n",
    "        expected_cost_reduction = 0\n",
    "        expected_cost_reduction_grad = 0\n",
    "        expected_cost_reduction_hess = 0\n",
    "\n",
    "        # looping backwards from N-1 to 1 using initial value of V_{N}\n",
    "        for i in reversed(range(0, self.n_timesteps_)):\n",
    "            # current variables\n",
    "            current_x = self.states_[i,:]\n",
    "            current_u = self.inputs_[i,:]\n",
    "\n",
    "            # updates to partial derivatives of cost function\n",
    "            l_xx = self.Q_k_\n",
    "            l_uu = self.R_k_\n",
    "\n",
    "            # l_ux = np.zeros((self.n_inputs_, self.n_states_))\n",
    "            l_x = self.Q_k_ @ np.zeros(self.n_states_).flatten()\n",
    "            l_u = self.R_k_ @ (current_u).flatten()\n",
    "\n",
    "            # get jacobian of discrete dynamics\n",
    "            A_k = self.A_(current_x, current_u, self.dt_, self.parameters_) # V'_x\n",
    "            B_k = self.B_(current_x, current_u, self.dt_, self.parameters_) # V'_u\n",
    "    \n",
    "            # all the Q vector/matrices\n",
    "            Q_x = l_x + A_k.T @ V_x # (5a)\n",
    "            Q_u = l_u + B_k.T @ V_x # (5b)\n",
    "            Q_ux = B_k.T @ V_xx @ A_k # (5c)\n",
    "            Q_uu = l_uu + B_k.T @ V_xx @ B_k # (5d)\n",
    "            Q_xx = l_xx + A_k.T @ V_xx @ A_k # (5e)\n",
    "\n",
    "            # compute and store gains\n",
    "            kSingValThreshold = 1e-4\n",
    "            (_,s,_) = np.linalg.svd(Q_uu)\n",
    "            if (np.min(s) < kSingValThreshold):\n",
    "                print(\"Q_uu is non-singular\")\n",
    "            Q_uu_inv = np.linalg.inv(Q_uu) # TODO: this can be singular, try using (9)\n",
    "            k = -Q_uu_inv @ Q_u # (6)\n",
    "            K = -Q_uu_inv @ Q_ux # (6)\n",
    "\n",
    "            k_trj[i,:] = k \n",
    "            K_trj[i,:,:] = K \n",
    "\n",
    "            # update the expected reduction (11a), delta V\n",
    "            # similar to equation of delta J(\\alpha)\n",
    "            current_cost_reduction_grad = -Q_u.T @ k\n",
    "            current_cost_reduction_hess = (0.5 * k.T @ (Q_uu) @ (k))\n",
    "            current_cost_reduction = current_cost_reduction_grad + current_cost_reduction_hess\n",
    "\n",
    "            expected_cost_reduction_grad += current_cost_reduction_grad\n",
    "            expected_cost_reduction_hess += current_cost_reduction_hess\n",
    "            expected_cost_reduction += current_cost_reduction\n",
    "\n",
    "            # update hessian and gradient of value function for the next iteration\n",
    "            V_x = Q_x + K.T @ Q_uu @ k + K.T @ Q_u + Q_ux.T @ k # (11b)\n",
    "            V_xx = Q_xx + K.T @ Q_uu @ K + K.T @ Q_ux + Q_ux.T @ K # (11c)\n",
    "\n",
    "        # store values\n",
    "        self.expected_cost_reduction_grad_ = expected_cost_reduction_grad\n",
    "        self.expected_cost_reduction_hess_ = expected_cost_reduction_hess\n",
    "        self.expected_cost_reduction_ = expected_cost_reduction\n",
    "\n",
    "        # print(\"Grad = {}, Hess = {}, Reduction = {}\".format(expected_cost_reduction_grad, expected_cost_reduction_hess, expected_cost_reduction))\n",
    "\n",
    "        # store gains\n",
    "        self.k_feedforward_ = k_trj\n",
    "        self.K_feedback_ = K_trj\n",
    "\n",
    "        return (k_trj, K_trj, expected_cost_reduction)\n",
    "\n",
    "    def forward_pass(self, learning_rate):\n",
    "        \"\"\"Forward pass of iLQR\n",
    "\n",
    "        Args:\n",
    "            learning_rate (double): learning rate (\\alpha)\n",
    "            \n",
    "        Returns:\n",
    "            ndarray: updated states, \\hat x (8a,b,c)\n",
    "            ndarray: updated inputs, \\hat u (12)\n",
    "        \"\"\"\n",
    "        # initialize before integration\n",
    "        states = np.zeros((self.n_timesteps_ + 1, self.n_states_)) # to store updated trajectory\n",
    "        inputs = np.zeros((self.n_timesteps_, self.n_inputs_)) # to store updated inputs\n",
    "        current_state = self.init_state_\n",
    "\n",
    "        # initialize and start integrating going forward for:\n",
    "        # state = [., x_1, x_2, ..., x_N]\n",
    "        # input =    [u_0, u_1, ..., u_{N-1}]\n",
    "        states[1,:] = current_state # (8a), assume that index 0 contains x_0\n",
    "        for i in range(0, self.n_timesteps_):\n",
    "            # use current gains\n",
    "            current_feedforward = learning_rate * self.k_feedforward_[i,:] # (12), \\hat represents the updated variables\n",
    "            current_feedback = self.K_feedback_[i,:,:] @ (current_state - self.states_[i,:]) # (12)\n",
    "            current_input = self.inputs_[i,:] + current_feedforward + current_feedback\n",
    "            # simulate\n",
    "            next_state = self.f_(current_state, current_input, self.dt_, self.parameters_).flatten()\n",
    "            # store states and inputs\n",
    "            states[i+1,:] = next_state\n",
    "            inputs[i,:] = current_input.flatten()\n",
    "            \n",
    "            # update states\n",
    "            current_state = next_state\n",
    "\n",
    "        return (states, inputs)\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"Solve all iLQR problem\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Solved states\n",
    "            ndarray: Solved inputs\n",
    "            ndarray: Solved feedforward gains\n",
    "            ndarray: Solved feedback gains\n",
    "            double: final cost\n",
    "        \"\"\"\n",
    "        # rollout with the initial guess\n",
    "        [states, inputs] = self.rollout()\n",
    "\n",
    "        # compute initial cost\n",
    "        current_cost = self.compute_cost(states, inputs)\n",
    "\n",
    "        # learning parameters\n",
    "        learning_speed = 0.95 # this can be modified, 0.95 is very slow\n",
    "        low_learning_rate = 0.05 # if learning rate drops to this value stop the optimization\n",
    "        low_expected_reduction = 1e-3 # determines optimality\n",
    "        \n",
    "        armijo_threshold = 0.1 # determines if current line search solve is good (labelled as 'c' in (13))\n",
    "\n",
    "        # start solving\n",
    "        for i in range(0, self.n_iterations_):\n",
    "            print(\"Starting iteration: {}, current cost: {}\".format(i, current_cost))\n",
    "            # backward pass\n",
    "            (k_feedforward, K_feedback, expected_reduction) = self.backward_pass()\n",
    "            # print statistics\n",
    "            print(\"Expected cost reduction = {}\".format(expected_reduction))\n",
    "            if (np.abs(expected_reduction) < low_expected_reduction):\n",
    "                # there is no further reduction, end the optimization\n",
    "                print(\"Stopping optimization and accepting solution, abs_expected_reduction = {}\".format(np.abs(expected_reduction)))\n",
    "                break\n",
    "            \n",
    "            # start forward pass and line search with \\alpha = 1\n",
    "            learning_rate = 1\n",
    "            armijo_flag = False\n",
    "            # execute line search until the armijo condition is met (for now just check if the cost decreased)\n",
    "            # TODO: add real armijo condition\n",
    "            while (learning_rate > low_learning_rate and armijo_flag == False):\n",
    "                # compute forward pass\n",
    "                (new_states, new_inputs) = self.forward_pass(learning_rate)\n",
    "                new_cost = self.compute_cost(new_states, new_inputs)\n",
    "                print(\"learning rate = {}, new cost = {}\".format(learning_rate, new_cost))\n",
    "\n",
    "                # compute armijo condition\n",
    "                cost_difference = (current_cost - new_cost)\n",
    "                expected_cost_red = learning_rate * (self.expected_cost_reduction_grad_ + learning_rate*self.expected_cost_reduction_hess_)\n",
    "                # if (expected_cost_red > 0):\n",
    "                #     armijo_flag = (cost_difference / expected_cost_red) > armijo_threshold # z (13)\n",
    "                # else:\n",
    "                #     armijo_flag = np.sign(expected_cost_red)\n",
    "                #     print(\".........................................\")\n",
    "\n",
    "                armijo_flag = (cost_difference / expected_cost_red) > armijo_threshold # z (13)\n",
    "\n",
    "                print(\"Learning rate: cost_difference = {}, expected_cost_red = {}, armijo_flag = {}\".format(\n",
    "                    cost_difference, expected_cost_red, armijo_flag))\n",
    "\n",
    "                if (armijo_flag):\n",
    "                    # accept new trajectory if armijo condition is met\n",
    "                    current_cost = new_cost\n",
    "                    self.states_ = new_states\n",
    "                    self.inputs_ = new_inputs\n",
    "                else:\n",
    "                    # no improvement, decrease learning rate and restart forward pass\n",
    "                    learning_rate = learning_speed*learning_rate\n",
    "\n",
    "            # if we exited while loop due to (learning_rate > low_learning_rate) being false\n",
    "            if (learning_rate < low_learning_rate):\n",
    "                print(\"Stopping optimization due to low learning rate\")\n",
    "                break\n",
    "\n",
    "        # return the current trajectory\n",
    "        states = self.states_\n",
    "        inputs = self.inputs_\n",
    "\n",
    "        return states, inputs, k_feedforward, K_feedback, current_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.0005]\n",
      "[0.     0.0005]\n",
      "Starting iteration: 0, current cost: 973.3712304334426\n",
      "Expected cost reduction = 1449.9257735600202\n",
      "learning rate = 1, new cost = 167.59117384226377\n",
      "Learning rate: cost_difference = 805.7800565911789, expected_cost_red = 1449.9257735600238, armijo_flag = True\n",
      "Starting iteration: 1, current cost: 167.59117384226377\n",
      "Expected cost reduction = 242.5227505316412\n",
      "learning rate = 1, new cost = 185.13594981831892\n",
      "Learning rate: cost_difference = -17.54477597605515, expected_cost_red = 242.52275053164092, armijo_flag = False\n",
      "learning rate = 0.95, new cost = 172.28921193975015\n",
      "Learning rate: cost_difference = -4.698038097486375, expected_cost_red = 226.55666945497455, armijo_flag = False\n",
      "learning rate = 0.9025, new cost = 159.1114010946356\n",
      "Learning rate: cost_difference = 8.479772747628175, expected_cost_red = 211.76328692827474, armijo_flag = False\n",
      "learning rate = 0.8573749999999999, new cost = 147.02378894654677\n",
      "Learning rate: cost_difference = 20.567384895716998, expected_cost_red = 198.0474645606701, armijo_flag = True\n",
      "Starting iteration: 2, current cost: 147.02378894654677\n",
      "Expected cost reduction = 104.70560574141977\n",
      "learning rate = 1, new cost = 68.0702493724377\n",
      "Learning rate: cost_difference = 78.95353957410907, expected_cost_red = 104.70560574141956, armijo_flag = True\n",
      "Starting iteration: 3, current cost: 68.0702493724377\n",
      "Expected cost reduction = 93.64221203996786\n",
      "learning rate = 1, new cost = 57.20206349437702\n",
      "Learning rate: cost_difference = 10.868185878060679, expected_cost_red = 93.64221203996769, armijo_flag = True\n",
      "Starting iteration: 4, current cost: 57.20206349437702\n",
      "Expected cost reduction = 78.03204475439864\n",
      "learning rate = 1, new cost = 51.20142091543117\n",
      "Learning rate: cost_difference = 6.00064257894585, expected_cost_red = 78.03204475439861, armijo_flag = False\n",
      "learning rate = 0.95, new cost = 45.151112350288365\n",
      "Learning rate: cost_difference = 12.050951144088657, expected_cost_red = 72.8949351414007, armijo_flag = True\n",
      "Starting iteration: 5, current cost: 45.151112350288365\n",
      "Expected cost reduction = 60.478647179384666\n",
      "learning rate = 1, new cost = 10.004930157827749\n",
      "Learning rate: cost_difference = 35.146182192460614, expected_cost_red = 60.47864717938481, armijo_flag = True\n",
      "Starting iteration: 6, current cost: 10.004930157827749\n",
      "Expected cost reduction = 8.11723495733861\n",
      "learning rate = 1, new cost = 4.60228682009211\n",
      "Learning rate: cost_difference = 5.402643337735639, expected_cost_red = 8.1172349573386, armijo_flag = True\n",
      "Starting iteration: 7, current cost: 4.60228682009211\n",
      "Expected cost reduction = 0.007468710703570741\n",
      "learning rate = 1, new cost = 4.597946915697201\n",
      "Learning rate: cost_difference = 0.0043399043949090554, expected_cost_red = 0.007468710703570733, armijo_flag = True\n",
      "Starting iteration: 8, current cost: 4.597946915697201\n",
      "Expected cost reduction = 0.0008065551627789487\n",
      "Stopping optimization and accepting solution, abs_expected_reduction = 0.0008065551627789487\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import dynamics\n",
    "(f,A,B) = symbolic_dynamics_pendulum()\n",
    "\n",
    "# initialize timing\n",
    "dt = 0.005\n",
    "start_time = 0\n",
    "end_time = 5\n",
    "time_span = np.arange(start_time, end_time, dt)\n",
    "\n",
    "# set states (start and end states are at rest)\n",
    "n_states = 2 # position and velocity\n",
    "n_inputs = 1 # inputs to the system\n",
    "init_state = np.array([0,0])\n",
    "target_state = np.array([np.pi,0])\n",
    "\n",
    "# initial guess\n",
    "initial_guess = 0.1*np.ones((time_span.shape[0], n_inputs))\n",
    "\n",
    "# define weights\n",
    "Q_k = np.zeros((n_states, n_states)) # just find a valid trajectory first\n",
    "R_k = 0.001*np.eye(n_inputs)\n",
    "Q_T = 100*np.eye(n_states)\n",
    "\n",
    "# physical parameters\n",
    "mass = 1\n",
    "gravity = 9.8\n",
    "pendulum_length = 1\n",
    "parameters = np.array([mass, gravity, pendulum_length])\n",
    "\n",
    "# iterations\n",
    "n_iterations = 50\n",
    "\n",
    "# ilqr\n",
    "ilqr = ilqr(init_state, target_state, initial_guess, dt, start_time, end_time, f, A, B, Q_k, R_k, Q_T, parameters, n_iterations)\n",
    "\n",
    "# Solve for swing up\n",
    "(states,inputs,k_feedforward,K_feedback,current_cost) = ilqr.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Backtracking_line_search\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a056c220e358cd254f9e086c9fbc1600e1f6c115100e0ffe1e1fae2263f3989e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hdrm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
