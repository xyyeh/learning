{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5eab17",
   "metadata": {},
   "source": [
    "### Gradient descent vs newtons method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6cc43b",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "1. Based on the observation that if the multi-variable function $F(\\boldsymbol x)$ is defined and differentiable in a neighborhood of a point $\\boldsymbol a$, then $F(\\boldsymbol x)$ decreases *fastest* if one goes from $\\boldsymbol a$ in the direction of the negative gradient of $F$ at $\\boldsymbol a$, $-\\nabla F(\\boldsymbol x)$, it follows that $\\boldsymbol a_{n+1} = \\boldsymbol a_{n} - \\gamma \\nabla F(\\boldsymbol a_{n})$ with $\\gamma \\in \\mathbb{R}_+$, then $F(\\boldsymbol a_{n}) \\geq F(\\boldsymbol a_{n+1})$\n",
    "2. The learning rate $\\gamma$ can be updated at every iteration. With certain assumptions on $F$. For example, with a convex $F$ and a lipschitz continuous $\\nabla F$, we can choose $\\gamma$ via a line search that satisfies **Wolfe** conditions or the **Barzilaiâ€“Borwein** method (https://en.wikipedia.org/wiki/Gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc721888-a031-4cbb-97ed-7739d22f0063",
   "metadata": {},
   "source": [
    "#### Newtons method\n",
    "1. Newton's method is different in that it is a root finding method, rather than a maximimization/minimization method. To get to the minimum or maximum, we formulate the problem to find the root of the derivative of the function, i.e. $F'(\\boldsymbol x)$\n",
    "2. Since newton's method requires the derivative of the input function, in this case, the derivative of $F'(\\boldsymbol x)$, i.e. $F''(\\boldsymbol x)$, the function to be considered needs to be twice-differentiable.\n",
    "3. Consider a univariate function $f(x)$ where $f:\\mathbb{R} \\rightarrow \\mathbb{R}$, we seek to solve $\\underset{x \\in \\mathbb{R}}{\\text{min}} f(x)$. Taking the second order taylor expansion of $f$ around $x_k$, we have $f(x_k+t) \\approx f(x_k)+f'(x_k)t+\\frac{1}{2}f''(x_k)t^2$. If the second derivative is positive, the quadratic approximation is a convex function of $t$, and its minimum can be found by setting the derivative to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
