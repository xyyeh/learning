{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Registration With Fphf & Maximal Cliques\n",
    "1. This is adapted from [CVPR2023's paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.html) on global registration.\n",
    "2. Unlike the conventional method of using RANSAC as the hypothesis generator, maximum cliques on graphs are used. Maximal cliques on graphs are regions of the data that captures \"interesting\" implicit relationships. Using the maximum cliques to generate hypothesis can be a robust solution for correspondence matching.\n",
    "3. Unlike RANSAC, the returned results of the same data remains consistent. The downside of using maximum clique is that significant time is required to find the cliques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 198835 points."
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from igraph import *\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "\n",
    "    # Create line segments to represent coordinate axes\n",
    "    axis_length = 1.0\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(np.array([[0, 0, 0], [axis_length, 0, 0], [0, axis_length, 0], [0, 0, axis_length]]))\n",
    "    line_set.lines = o3d.utility.Vector2iVector(np.array([[0, 1], [0, 2], [0, 3]]))\n",
    "    line_set.colors = o3d.utility.Vector3dVector(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))  # RGB colors for X, Y, Z axes\n",
    "\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp, line_set],\n",
    "                                      zoom=0.4559,\n",
    "                                      front=[0.6452, -0.3036, -0.7011],\n",
    "                                      lookat=[1.9892, 2.0208, 1.8945],\n",
    "                                      up=[-0.2779, -0.9482, 0.1556])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = o3d.data.DemoICPPointClouds()\n",
    "src_pcd = o3d.io.read_point_cloud(data.paths[0])\n",
    "tgt_pcd = o3d.io.read_point_cloud(data.paths[1])\n",
    "trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "                            [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "src_pcd.transform(trans_init)\n",
    "# draw_registration_result(src_pcd, tgt_pcd, np.identity(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(pts, trans):\n",
    "    if len(pts.shape) == 3:\n",
    "        trans_pts = torch.einsum('bnm,bmk->bnk', trans[:, :3, :3],\n",
    "                                 pts.permute(0, 2, 1)) + trans[:, :3, 3:4]\n",
    "        return trans_pts.permute(0, 2, 1)\n",
    "    else:\n",
    "        trans_pts = torch.einsum('nm,mk->nk', trans[:3, :3],\n",
    "                                 pts.T) + trans[:3, 3:4]\n",
    "        return trans_pts.T\n",
    "    \n",
    "def integrate_trans(R, t):\n",
    "    \"\"\"\n",
    "    Integrate SE3 transformations from R and t, support torch.Tensor and np.ndarry.\n",
    "    Input\n",
    "        - R: [3, 3] or [bs, 3, 3], rotation matrix\n",
    "        - t: [3, 1] or [bs, 3, 1], translation matrix\n",
    "    Output\n",
    "        - trans: [4, 4] or [bs, 4, 4], SE3 transformation matrix\n",
    "    \"\"\"\n",
    "    if len(R.shape) == 3:\n",
    "        if isinstance(R, torch.Tensor):\n",
    "            trans = torch.eye(4)[None].repeat(R.shape[0], 1, 1).to(R.device)\n",
    "        else:\n",
    "            trans = np.eye(4)[None]\n",
    "        trans[:, :3, :3] = R\n",
    "        trans[:, :3, 3:4] = t.view([-1, 3, 1])\n",
    "    else:\n",
    "        if isinstance(R, torch.Tensor):\n",
    "            trans = torch.eye(4).to(R.device)\n",
    "        else:\n",
    "            trans = np.eye(4)\n",
    "        trans[:3, :3] = R\n",
    "        trans[:3, 3:4] = t\n",
    "    return trans\n",
    "\n",
    "def rigid_transform_3d(A, B, weights=None, weight_threshold=0):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - A:       [bs, num_corr, 3], source point cloud\n",
    "        - B:       [bs, num_corr, 3], target point cloud\n",
    "        - weights: [bs, num_corr]     weight for each correspondence\n",
    "        - weight_threshold: float,    clips points with weight below threshold\n",
    "    Output:\n",
    "        - R, t\n",
    "    \"\"\"\n",
    "    bs = A.shape[0]\n",
    "    if weights is None:\n",
    "        weights = torch.ones_like(A[:, :, 0])\n",
    "    weights[weights < weight_threshold] = 0\n",
    "    # weights = weights / (torch.sum(weights, dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "    # find mean of point cloud\n",
    "    centroid_A = torch.sum(A * weights[:, :, None], dim=1, keepdim=True) / (\n",
    "            torch.sum(weights, dim=1, keepdim=True)[:, :, None] + 1e-6)\n",
    "    centroid_B = torch.sum(B * weights[:, :, None], dim=1, keepdim=True) / (\n",
    "            torch.sum(weights, dim=1, keepdim=True)[:, :, None] + 1e-6)\n",
    "\n",
    "    # subtract mean\n",
    "    Am = A - centroid_A\n",
    "    Bm = B - centroid_B\n",
    "\n",
    "    # construct weight covariance matrix\n",
    "    Weight = torch.diag_embed(weights)  # 升维度，然后变为对角阵\n",
    "    H = Am.permute(0, 2, 1) @ Weight @ Bm  # permute : tensor中的每一块做转置\n",
    "\n",
    "    # find rotation\n",
    "    U, S, Vt = torch.svd(H.cpu())\n",
    "    U, S, Vt = U.to(weights.device), S.to(weights.device), Vt.to(weights.device)\n",
    "    delta_UV = torch.det(Vt @ U.permute(0, 2, 1))\n",
    "    eye = torch.eye(3)[None, :, :].repeat(bs, 1, 1).to(A.device)\n",
    "    eye[:, -1, -1] = delta_UV\n",
    "    R = Vt @ eye @ U.permute(0, 2, 1)\n",
    "    t = centroid_B.permute(0, 2, 1) - R @ centroid_A.permute(0, 2, 1)\n",
    "    # warp_A = transform(A, integrate_trans(R,t))\n",
    "    # RMSE = torch.sum( (warp_A - B) ** 2, dim=-1).mean()\n",
    "    return integrate_trans(R, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract features using fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Downsample with a voxel size 0.100.\n",
      "Time taken: 0.011176 seconds\n",
      ":: Estimate normal with search radius 0.200.\n",
      "Time taken: 0.022341 seconds\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      "Time taken: 0.020528 seconds\n",
      ":: Downsample with a voxel size 0.100.\n",
      "Time taken: 0.006227 seconds\n",
      ":: Estimate normal with search radius 0.200.\n",
      "Time taken: 0.015871 seconds\n",
      ":: Compute FPFH feature with search radius 0.500.\n",
      "Time taken: 0.015428 seconds\n"
     ]
    }
   ],
   "source": [
    "def extract_fpfh_features(pcd, voxel_size):\n",
    "    start_time = time.perf_counter()\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    radius_normal = voxel_size * 2\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30)\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100),\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "    features = np.array(pcd_fpfh.data).T\n",
    "    features = features / (\n",
    "        np.linalg.norm(features, axis=1, keepdims=True) + 1e-6\n",
    "    )  # rescale to 0~1\n",
    "\n",
    "    return pcd_down, features\n",
    "\n",
    "\n",
    "voxel_size = 0.1\n",
    "src_kpts, src_desc = extract_fpfh_features(src_pcd, voxel_size)\n",
    "tgt_kpts, tgt_desc = extract_fpfh_features(tgt_pcd, voxel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Graph construction: 1.16ms\n",
      "Search maximal cliques: 151.45ms\n",
      "Total: 16765\n",
      "After filtered: 941\n",
      "Time taken for maximal clique: 0.491874 seconds\n",
      "Transformation = \n",
      "[[-0.54238254  0.83967847  0.02759058  0.6066551 ]\n",
      " [-0.21372691 -0.1696663   0.96204674  0.86871016]\n",
      " [ 0.8124912   0.51590043  0.27148592 -1.4847856 ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "distance = np.sqrt(2 - 2 * (src_desc @ tgt_desc.T) + 1e-6)\n",
    "source_idx = np.argmin(distance, axis=1)  # for each row save the index of minimum\n",
    "\n",
    "# feature matching\n",
    "corr = np.concatenate(\n",
    "    [np.arange(source_idx.shape[0])[:, None], source_idx[:, None]], axis=-1\n",
    ")  # n to 1\n",
    "\n",
    "src_pts = np.array(src_kpts.points, dtype=np.float32)[corr[:,0]]\n",
    "tgt_pts = np.array(tgt_kpts.points, dtype=np.float32)[corr[:,1]]\n",
    "\n",
    "src_pts = torch.from_numpy(src_pts).cuda()\n",
    "tgt_pts = torch.from_numpy(tgt_pts).cuda()\n",
    "\n",
    "# graph construction\n",
    "t1 = time.perf_counter()\n",
    "inlier_threshold = 0.1\n",
    "src_dist = ((src_pts[:, None, :] - src_pts[None, :, :]) ** 2).sum(-1) ** 0.5\n",
    "tgt_dist = ((tgt_pts[:, None, :] - tgt_pts[None, :, :]) ** 2).sum(-1) ** 0.5\n",
    "cross_dis = torch.abs(src_dist - tgt_dist)\n",
    "FCG = torch.clamp(1 - cross_dis ** 2 / inlier_threshold ** 2, min=0)\n",
    "FCG = FCG - torch.diag_embed(torch.diag(FCG))\n",
    "FCG[FCG < 0.99] = 0\n",
    "SCG = torch.matmul(FCG, FCG) * FCG\n",
    "t2 = time.perf_counter()\n",
    "print(f':: Graph construction: %.2fms' % ((t2 - t1) * 1000))\n",
    "\n",
    "# search cliques\n",
    "SCG = SCG.cpu().numpy()\n",
    "t1 = time.perf_counter()\n",
    "graph = Graph.Adjacency((SCG > 0).tolist())\n",
    "graph.es['weight'] = SCG[SCG.nonzero()]\n",
    "graph.vs['label'] = range(0, corr.shape[0])\n",
    "graph.to_undirected()\n",
    "\n",
    "macs = graph.maximal_cliques(min=3)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Search maximal cliques: %.2fms\" % ((t2 - t1) * 1000))\n",
    "print(f\"Total: %d\" % len(macs))\n",
    "clique_weight = np.zeros(len(macs), dtype=float)\n",
    "for ind in range(len(macs)):\n",
    "    mac = list(macs[ind])\n",
    "    if len(mac) >= 3:\n",
    "        for i in range(len(mac)):\n",
    "            for j in range(i + 1, len(mac)):\n",
    "                clique_weight[ind] = clique_weight[ind] + SCG[mac[i], mac[j]]\n",
    "\n",
    "clique_ind_of_node = np.ones(corr.shape[0], dtype=int) * -1\n",
    "max_clique_weight = np.zeros(corr.shape[0], dtype=float)\n",
    "max_size = 3\n",
    "for ind in range(len(macs)):\n",
    "    mac = list(macs[ind])\n",
    "    weight = clique_weight[ind]\n",
    "    if weight > 0:\n",
    "        for i in range(len(mac)):\n",
    "            if weight > max_clique_weight[mac[i]]:\n",
    "                max_clique_weight[mac[i]] = weight\n",
    "                clique_ind_of_node[mac[i]] = ind\n",
    "                max_size = len(mac) > max_size and len(mac) or max_size\n",
    "\n",
    "filtered_clique_ind = list(set(clique_ind_of_node))\n",
    "try:\n",
    "    filtered_clique_ind.remove(-1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f'After filtered: %d' % len(filtered_clique_ind))\n",
    "group = []\n",
    "for s in range(3, max_size + 1):\n",
    "    group.append([])\n",
    "for ind in filtered_clique_ind:\n",
    "    mac = list(macs[ind])\n",
    "    group[len(mac) - 3].append(ind)\n",
    "\n",
    "tensor_list_A = []\n",
    "tensor_list_B = []\n",
    "for i in range(len(group)):\n",
    "    if len(group[i]) == 0:\n",
    "        continue\n",
    "    batch_A = src_pts[list(macs[group[i][0]])][None]\n",
    "    batch_B = tgt_pts[list(macs[group[i][0]])][None]\n",
    "    if len(group) == 1:\n",
    "        continue\n",
    "    for j in range(1, len(group[i])):\n",
    "        mac = list(macs[group[i][j]])\n",
    "        src_corr = src_pts[mac][None]\n",
    "        tgt_corr = tgt_pts[mac][None]\n",
    "        batch_A = torch.cat((batch_A, src_corr), 0)\n",
    "        batch_B = torch.cat((batch_B, tgt_corr), 0)\n",
    "    tensor_list_A.append(batch_A)\n",
    "    tensor_list_B.append(batch_B)\n",
    "\n",
    "max_score = 0\n",
    "final_trans = torch.eye(4)\n",
    "for i in range(len(tensor_list_A)):\n",
    "    trans = rigid_transform_3d(tensor_list_A[i], tensor_list_B[i], None, 0)\n",
    "    pred_tgt = transform(src_pts[None], trans)  # [bs,  num_corr, 3]\n",
    "    L2_dis = torch.norm(pred_tgt - tgt_pts[None], dim=-1)  # [bs, num_corr]\n",
    "    MAE_score = torch.div(torch.sub(inlier_threshold, L2_dis), inlier_threshold)\n",
    "    MAE_score = torch.sum(MAE_score * (L2_dis < inlier_threshold), dim=-1)\n",
    "    max_batch_score_ind = MAE_score.argmax(dim=-1)\n",
    "    max_batch_score = MAE_score[max_batch_score_ind]\n",
    "    if max_batch_score > max_score:\n",
    "        max_score = max_batch_score\n",
    "        final_trans = trans[max_batch_score_ind]\n",
    "\n",
    "def post_refinement(\n",
    "    initial_trans, src_kpts, tgt_kpts, iters, inlier_threshold=0.1, weights=None\n",
    "):\n",
    "    pre_inlier_count = 0\n",
    "    for i in range(iters):\n",
    "        pred_tgt = transform(src_kpts, initial_trans)\n",
    "        L2_dis = torch.norm(pred_tgt - tgt_kpts, dim=-1)\n",
    "        pred_inlier = (L2_dis < inlier_threshold)[0]\n",
    "        inlier_count = torch.sum(pred_inlier)\n",
    "        if inlier_count <= pre_inlier_count:\n",
    "            break\n",
    "        pre_inlier_count = inlier_count\n",
    "        initial_trans = rigid_transform_3d(\n",
    "            A=src_kpts[:, pred_inlier, :],\n",
    "            B=tgt_kpts[:, pred_inlier, :],\n",
    "            weights=1 / (1 + (L2_dis / inlier_threshold) ** 2)[:, pred_inlier],\n",
    "        )\n",
    "    return initial_trans\n",
    "\n",
    "\n",
    "final_trans1 = post_refinement(\n",
    "    final_trans[None], src_pts[None], tgt_pts[None], 20, inlier_threshold\n",
    ")\n",
    "\n",
    "final_trans_cpu = final_trans1.cpu().numpy()\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Time taken for maximal clique: {elapsed_time:.6f} seconds\")\n",
    "print(f\"Transformation = \\n{final_trans_cpu[0]}\")\n",
    "# draw_registration_result(src_kpts, tgt_kpts, final_trans_cpu[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_open3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
