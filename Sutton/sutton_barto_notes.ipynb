{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1 Introduction\n",
    "1. Reinforcement learning cannot be categorized as supervised or unsupervised learning. Supervised learning requires labeled examples while reinforcement learning is reward based and its learning through exploration and exploitation to maximize rewards. Unsupervised learning is typically about finding structure hidden in collections of unlabeled data.\n",
    "\n",
    "2. On stochastic tasks, an agent trained via RL needs to try an action multiple times to gain a reliable estimate of its expected reward. In many robotic manipulation tasks, the uncertainties and dynamics in the environment renders the task stochastic.\n",
    "\n",
    "3. RL can also work on continuous-time systems, though more difficult to solve. Unlike the game of tic-tac-toe that is finite in its states, RL is just as applicable when behavior continues indefinitely and when rewards of various magnitudes can be received at any time.\n",
    "\n",
    "4. RL by no meails entails a tabula rasa view of learning and intelligence. Prior information can be incorporated into RL. In situations when part of the state is hidden or when different states appear, RL can be applied as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3 Finite MDP\n",
    "1. MDPs involve delayed reward and the need to trade off immediate and delay reward.\n",
    "\n",
    "2. In an MDP, an agent at time $t=0$, receives information of the state $S_0$ and on that basis selects an action $A_0$. One time step latter, in part as a consequence of its action, the agent receives a reward $R_1$ and finds itself in a new state $S_1$. The overall trajectory for this interaction is $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, ...$\n",
    "\n",
    "3. In finite MDPs, the sets of states, actions and rewards consist of finite elements. The dynamics of of the MDP can be described as a probability distribution $p:S \\times R \\times S \\times A \\rightarrow [0,1]$: $$p(s',r | s,a) \\doteq Pr \\{ S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a \\} $$ $$\\sum_{s' \\in S}^{} \\sum_{r \\in R}^{} p(s',r|s,a)=1, \\forall s \\in S, \\forall a \\in A(s)$$\n",
    "\n",
    "4. The *state-transition probabilities* $p:S \\times S \\times A \\rightarrow [0,1]$ can be defined as: $$p(s'|s,a) \\doteq Pr \\{ S_t=s'| S_{t-1} = s, A_{t-1} = a \\} = \\sum_{r \\in R}^{} p(s',r|s,a)$$\n",
    "\n",
    "5. We can also compute the expected rewards for the state-action pair $r:S \\times A \\rightarrow \\mathbb{R}$: $$r(s,a) \\doteq \\mathbb{E} [R_t|S_{t-1}=s, A_{t-1}=a] = \\sum_{r \\in R}^{} r \\sum_{s' \\in S}^{} p(s',r|s,a)$$\n",
    "\n",
    "6. Similarly, the expected rewards for the state-action-next-state triples $r:S \\times A \\times S \\rightarrow \\mathbb{R}$ is defined as $$r(s,a,s') \\doteq \\mathbb{E} [R_t|S_{t-1}=s, A_{t-1}=a, S_{t}=s'] = \\sum_{r \\in R} r \\frac{p(s',r|s,a)}{p(s'|s,a)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
